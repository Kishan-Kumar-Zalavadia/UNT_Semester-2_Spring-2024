{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Activity 3: Normalization**\n",
        "**Instructions:**\n",
        "\n",
        "---\n",
        "* Please download the provided IPython Notebook (ipynb) file and open it in Google Colab. Once opened, enter your code in the same file directly beneath the relevant question's code block.\n",
        "* Insert a text block below your code to briefly explain it, mentioning any libraries or functions utilized. Conclude your activity with a comprehensive explanation of your overall approach, aiming for about 200 words, in the final section of the notebook.\n",
        "\n",
        "* Submit  \n",
        "1. The IPython Notebook (ipynb) file.  \n",
        "2. A PDF version of the notebook (converted from ipynb).\n",
        "\n",
        "* The similarity score should be less than 15%"
      ],
      "metadata": {
        "id": "Ur1Spl3neyQc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8hFNQohPORn"
      },
      "source": [
        "# **Text Preprocessing Beyond Tokenization**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Word** **normalization**\n",
        "**Word normalization** is a text preprocessing technique used in natural language processing (NLP) to standardize and simplify words or tokens in a text document. The goal of word normalization is to make text data more consistent and manageable for analysis. This process can involve various transformations, such as converting all text to lowercase, removing punctuation, expanding contractions, and performing tasks like stemming or lemmatization to reduce words to their base or dictionary forms. Word normalization helps improve the accuracy and effectiveness of NLP tasks by reducing the complexity of text data and ensuring that similar words are treated as equivalent."
      ],
      "metadata": {
        "id": "HZIYqdWPmHr_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "peiyYN47rMy1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27e57e86-d749-4f5c-a59b-ec34344d3334"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# for using NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# for using SpaCy\n",
        "import spacy\n",
        "\n",
        "# for HuggingFace\n",
        "!pip install transformers\n",
        "# !pip install ftfy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Import/Download the required libraries/models"
      ],
      "metadata": {
        "id": "yAcE8IDOvnsZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y3Bh5T8sJXFQ"
      },
      "outputs": [],
      "source": [
        "# trick to wrap text to the viewing window for this notebook\n",
        "# Ref: https://stackoverflow.com/questions/58890109/line-wrapping-in-collaboratory-google-results\n",
        "#helps improve the readability and formatting of code output in the notebook.\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Revisiting Tokenization** :**TreebankWordTokenizer**\n",
        "\n",
        "The **Treebank Word Tokenizer** is a text processing tool used in natural language processing (NLP) to split text into individual words or tokens. It follows the tokenization conventions and standards of the Penn Treebank corpus"
      ],
      "metadata": {
        "id": "Id3muvWNlNj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "text=\"Hello everyone. Welcome to NLP Course.\"\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KiRKo3DKlfuQ",
        "outputId": "4e21332a-3e32-4e17-fb15-764b78f0df2b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'everyone.', 'Welcome', 'to', 'NLP', 'Course', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- TreebankWordTokenizer splits the sentence into words called as tokens)"
      ],
      "metadata": {
        "id": "loCc-_k-wAGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Using Regular Expression**\n",
        "\n",
        "**RegexpTokenizer** is a text processing tool provided by the Natural Language Toolkit (NLTK) library in Python. It is used to tokenize (split) text into individual tokens (words or phrases) based on a specified regular expression pattern."
      ],
      "metadata": {
        "id": "43OOaeg7o78C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"[\\w']+\")#[\\w']+ as a whole matches sequences of word characters (letters, digits, underscores) and single quotes in a string\n",
        "text = \"Let's see how it's working. We also have digits like 123 and 010\"\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "me379siro8Jp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "cad14a59-d211-4039-8480-c56a55c54737"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Let's\",\n",
              " 'see',\n",
              " 'how',\n",
              " \"it's\",\n",
              " 'working',\n",
              " 'We',\n",
              " 'also',\n",
              " 'have',\n",
              " 'digits',\n",
              " 'like',\n",
              " '123',\n",
              " 'and',\n",
              " '010']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** Copy the above snippet code and Modify the above regular expression to match only **digits** from the above given text.  "
      ],
      "metadata": {
        "id": "jaHfuJo-phTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE HERE\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"[\\d]+\")#[\\w']+ as a whole matches sequences of word characters (letters, digits, underscores) and single quotes in a string\n",
        "text = \"Let's see how it's working. We also have digits like 123 and 010\"\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "ZiQuSrzQphoo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "89ee110c-b58b-4dca-ae65-bd976ecdfa88"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['123', '010']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In this modified regular expression '\\d' matches any digit and '+' means digit occurs one or more times."
      ],
      "metadata": {
        "id": "CRLs-mKfwrNT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgaX-Ck7YYzY"
      },
      "source": [
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNUmlAHIQ9gs"
      },
      "source": [
        "## **(Tutorial) Stemming and Lemmatization using NLTK**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming** is a text normalization technique in natural language processing and information retrieval. It involves reducing words to their root or base form, often by removing suffixes or prefixes. The goal of stemming is to convert words with the same meaning but different forms into a common base form so that they can be treated as equivalent during text analysis and retrieval. Stemming helps improve information retrieval and text processing tasks by reducing the complexity of words while maintaining their core meaning. Common stemming algorithms include the Porter Stemmer and Snowball Stemmer."
      ],
      "metadata": {
        "id": "Rxa-khLuaEZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Porter Stemmer**\n",
        "The **Porter Stemmer** is a well-known algorithm for stemming in natural language processing. It was  designed to reduce words to their root or base form by removing common suffixes. Stemming is the process of reducing words to their linguistic root or base form to simplify text analysis and improve information retrieval.For example, it can convert words like \"running,\" \"runs,\" and \"ran\" to their common root \"run.\""
      ],
      "metadata": {
        "id": "7TPOS73taHEq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jCebsYwiQxf"
      },
      "source": [
        "Let's see how we can perform stemming and lemmatization using NLTK library..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "k3M0mDUIiSdu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "4e6b4fb2-652f-4b2e-8cc6-45ba7c319d56"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'cats' after stemming: cat\n",
            "'better' after stemming: better\n",
            "'abaci' after stemming: abaci\n",
            "'aardwolves' after stemming: aardwolv\n",
            "'generically' after stemming: gener\n"
          ]
        }
      ],
      "source": [
        "# importing PorterStemmer class from nltk.stem module\n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()    # instantiating an object of the PorterStemmer class\n",
        "\n",
        "stem = porter.stem('cats')    # calling the stemmer algorithm on the desired word\n",
        "print(f\"'cats' after stemming: {stem}\")\n",
        "\n",
        "stem = porter.stem('better')\n",
        "print(f\"'better' after stemming: {stem}\")\n",
        "\n",
        "stem = porter.stem('abaci')\n",
        "print(f\"'abaci' after stemming: {stem}\")\n",
        "\n",
        "stem = porter.stem('aardwolves')\n",
        "print(f\"'aardwolves' after stemming: {stem}\")\n",
        "\n",
        "stem = porter.stem('generically')\n",
        "print(f\"'generically' after stemming: {stem}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Lemmatization**\n",
        "**Lemmatization** is a natural language processing technique that reduces words to their base or dictionary form, known as a \"lemma.\" Unlike stemming, which often involves removing suffixes to approximate a word's root, lemmatization considers the word's context and grammatical meaning. The goal is to transform different inflected forms of a word into a common base form. Lemmatization is particularly useful for maintaining the grammatical correctness of words in text analysis and information retrieval tasks."
      ],
      "metadata": {
        "id": "WOhnG5IIcaHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**WordNet Lemmatizer**\n",
        "The **WordNet Lemmatizer** is a lemmatization tool based on WordNet, a lexical database of the English language. WordNet groups words into sets of synonyms called \"synsets\" and provides a rich lexical and semantic structure for the English language. The WordNet Lemmatizer uses this semantic information to perform lemmatization, which is the process of reducing words to their base or dictionary form (lemma)"
      ],
      "metadata": {
        "id": "tURX72sldJvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "8wVrYUPXS2Oy",
        "outputId": "0c1b4f79-d842-430c-d969-6c0a5dfcbcfe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Download the WordNet from NLTK."
      ],
      "metadata": {
        "id": "grOFxtr6yWTn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6Jo-qgQfjuli",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "901cd7bc-7c82-4a8b-9327-1a21830afb44"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'cats' after lemmatization: cat\n",
            "'better' after lemmatization: better\n",
            "'abaci' after lemmatization: abacus\n",
            "'aardwolves' after lemmatization: aardwolf\n",
            "'generically' after lemmatization: generically\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "'better' (as an adjective) after lemmatization: good\n"
          ]
        }
      ],
      "source": [
        "# importing WordNet-based lemmatizer class from nltk.stem module\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()    # instantiating an object of the WordNetLemmatizer class\n",
        "\n",
        "lemma = lemmatizer.lemmatize('cats')    # calling the lemmatization algorithm on the desired word\n",
        "print(f\"'cats' after lemmatization: {lemma}\")\n",
        "\n",
        "lemma = lemmatizer.lemmatize('better')\n",
        "print(f\"'better' after lemmatization: {lemma}\")\n",
        "\n",
        "lemma = lemmatizer.lemmatize('abaci')\n",
        "print(f\"'abaci' after lemmatization: {lemma}\")\n",
        "\n",
        "lemma = lemmatizer.lemmatize('aardwolves')\n",
        "print(f\"'aardwolves' after lemmatization: {lemma}\")\n",
        "\n",
        "lemma = lemmatizer.lemmatize('generically')\n",
        "print(f\"'generically' after lemmatization: {lemma}\")\n",
        "\n",
        "print(\"\\n\\n\\n\")\n",
        "lemma = lemmatizer.lemmatize('better', pos='a')   # 'a' denoted ADJECTIVE part-of-speech\n",
        "print(f\"'better' (as an adjective) after lemmatization: {lemma}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDGVeoHqrQkF"
      },
      "source": [
        "### **Stemming on text string**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def stem_text(text):\n",
        "    # Initialize the Porter Stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Apply the stemmer to each word and join them back into a text\n",
        "    stemmed_text = ' '.join([stemmer.stem(word) for word in words])\n",
        "\n",
        "    return stemmed_text\n",
        "\n",
        "# Example usage:\n",
        "text = \"He is jumping, and he jumped over the jumps.\"\n",
        "stemmed_text = stem_text(text)\n",
        "print(stemmed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0OILkAMtu6iS",
        "outputId": "80e75754-f0d5-4ded-d0dc-4c7d1ac334c8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "he is jump , and he jump over the jump .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TCtK0QouYj2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "c5b5415d-2e41-45f0-bff7-2cd060a124fe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given text:\n",
            "In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form; generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root.\n"
          ]
        }
      ],
      "source": [
        "# This is the text on which you have to perform stemming; taken from Wikipedia.\n",
        "text = \"In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form; generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root.\"\n",
        "print(\"Given text:\")\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tutorial 2**"
      ],
      "metadata": {
        "id": "Cl9AIuruwUrE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "el7w7c7HmY9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "0dbe7cf3-c454-4218-9e9d-7b530836af8b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After punctuation removal:\n",
            "in linguistic morphology and information retrieval stemming is the process of reducing inflected or sometimes derived words to their word stem base or root form generally a written word form the stem need not be identical to the morphological root of the word it is usually sufficient that related words map to the same stem even if this stem is not in itself a valid root\n",
            "\n",
            "\n",
            "After stopword removal:\n",
            "['linguistic', 'morphology', 'information', 'retrieval', 'stemming', 'process', 'reducing', 'inflected', 'sometimes', 'derived', 'words', 'word', 'stem', 'base', 'root', 'form', 'generally', 'written', 'word', 'form', 'stem', 'need', 'identical', 'morphological', 'root', 'word', 'usually', 'sufficient', 'related', 'words', 'map', 'stem', 'even', 'stem', 'valid', 'root']\n"
          ]
        }
      ],
      "source": [
        "#CODE BLOCK 1\n",
        "en_stopwords = set(stopwords.words('english'))\n",
        "def remove_punc(text_string):\n",
        "  return re.sub('[^a-zA-Z0-9 ]', '', text_string.lower())\n",
        "\n",
        "def remove_stopwords(text_string):\n",
        "  return [ token for token in text_string.split(' ') if token not in en_stopwords ]\n",
        "\n",
        "# applying punctuation removal to the text\n",
        "unpunc_text = remove_punc(text)\n",
        "print(\"After punctuation removal:\")\n",
        "print(unpunc_text)\n",
        "\n",
        "# # applying stopword removal to the text\n",
        "clean_text = remove_stopwords(unpunc_text)\n",
        "print(\"\\n\\nAfter stopword removal:\")\n",
        "print(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [^a-zA-Z0-9 ]: The means other than characters other than lowercase, uppercase, digits, and space\n",
        "-The above code removed all the punctuations and stop words."
      ],
      "metadata": {
        "id": "wTvMLyAlzbqp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac5uju7eGVfg"
      },
      "source": [
        "#### **Question 2. Perform stemming on the cleaned text(from tutorial2-code block 1) above using the Porter Stemmer from NLTK.**\n",
        "\n",
        "**Hint:** import PorterStemmer from nltk.stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jAC8FFLCErdI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "e8d2d70d-957b-4fb4-f6d7-b61db97a36c3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Stemming: \n",
            "linguistic morphology information retrieval stemming process reducing inflected sometimes derived words word stem base root form generally written word form stem need identical morphological root word usually sufficient related words map stem even stem valid root\n",
            "\n",
            "After Stemming: \n",
            "linguist morpholog inform retriev stem process reduc inflect sometim deriv word word stem base root form gener written word form stem need ident morpholog root word usual suffici relat word map stem even stem valid root\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def stem_text(text):\n",
        "    # Initialize the Porter Stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Apply the stemmer to each word and join them back into a text\n",
        "    stemmed_text = ' '.join([stemmer.stem(word) for word in words])\n",
        "\n",
        "    return stemmed_text\n",
        "\n",
        "# Example usage:\n",
        "print(\"Before Stemming: \\n\"+' '.join(clean_text))\n",
        "print()\n",
        "stemmed_text = stem_text(' '.join(clean_text))\n",
        "print(\"After Stemming: \\n\"+stemmed_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The above code applies stemming on the clean_text generated from tutorial-2.\n",
        "- For stemming, the code uses PorterStemmer."
      ],
      "metadata": {
        "id": "Kejfut4z10xh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization on text string**"
      ],
      "metadata": {
        "id": "4bUoRnNN6t0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "ib7DcVk3r9Aj",
        "outputId": "2d35460f-63fc-4875-8faf-a55008013af9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example text to lemmatize\n",
        "text = \"There are like more than 100 foxes and lions in this forest.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = text.split()\n",
        "\n",
        "# Lemmatize each word and join them back into a sentence\n",
        "lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
        "\n",
        "# Print the original and lemmatized text\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Lemmatized Text:\", lemmatized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "fp2nzJba6sJZ",
        "outputId": "424f176c-2010-4f1c-a56c-eec5e9756a2e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: There are like more than 100 foxes and lions in this forest.\n",
            "Lemmatized Text: There are like more than 100 fox and lion in this forest.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N44TNDasS6eK"
      },
      "source": [
        "#### **Question 3. Perform lemmatization on the same cleaned text(from tutorial2-code block 1) above using NLTK's lemmatizer.**\n",
        "\n",
        "**Hint**:import WordNetLemmatizer from nltk.stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tJ-moA25Erh3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "62ca65a0-1268-4c93-8007-f22e24be6ca8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: linguistic morphology information retrieval stemming process reducing inflected sometimes derived words word stem base root form generally written word form stem need identical morphological root word usually sufficient related words map stem even stem valid root\n",
            "Lemmatized Text: linguistic morphology information retrieval stemming process reducing inflected sometimes derived word word stem base root form generally written word form stem need identical morphological root word usually sufficient related word map stem even stem valid root\n"
          ]
        }
      ],
      "source": [
        "# apply NLTK's lemmatizer on the cleaned text (after punctuation and stopwords are removed) below this comment\n",
        "#CODE HERE\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# # Example text to lemmatize\n",
        "# text = \"There are like more than 100 foxes and lions in this forest.\"\n",
        "\n",
        "# # Tokenize the text into words\n",
        "# words = text.split()\n",
        "\n",
        "# Lemmatize each word and join them back into a sentence\n",
        "lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in clean_text])\n",
        "\n",
        "# Print the original and lemmatized text\n",
        "print(\"Original Text:\", ' '.join(clean_text))\n",
        "print(\"Lemmatized Text:\", lemmatized_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The above code usesWordNetLemmatizer for lemmatization.\n",
        "- It lemmatizes the clean_text.\n",
        "- Since the clean_text is already split in words we do not need to split it again."
      ],
      "metadata": {
        "id": "hnsrft4g39hi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJNe9XfmOcqi"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYE8A-Mk4BKe"
      },
      "source": [
        "## **(Tutorial) Subword Tokenization using HuggingFace**\n",
        "\n",
        "**Hugging Face** is used for subword tokenization by offering NLP practitioners access to pre-trained subword tokenizers and models. Hugging Face's \"transformers\" library offers pre-trained models and tokenizers, such as Byte Pair Encoding (BPE) and SentencePiece, which are widely used for subword tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Subword tokenization** is a text processing technique used in natural language processing (NLP) to break down words into smaller units, often subword pieces. This approach is particularly useful for handling languages with complex morphology or when dealing with out-of-vocabulary words. Subword tokenization methods like Byte-Pair Encoding (BPE) and SentencePiece divide text into subword units, such as character-level tokens or subword pieces, allowing NLP models to work with a more extensive and adaptable vocabulary. This technique improves the handling of rare words and enhances the performance of NLP models on a wide range of languages and tasks"
      ],
      "metadata": {
        "id": "Y48GHrsfgHI5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bV2KUGEhIm2M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        },
        "outputId": "f00654af-435d-4660-da1b-41483aa266e7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.1)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2024.2.2)\n",
            "--2024-02-09 22:31:32--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-vocab.json\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.108.214, 54.231.128.72, 16.182.32.8, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.108.214|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1042301 (1018K) [application/json]\n",
            "Saving to: ‘gpt2-medium-vocab.json’\n",
            "\n",
            "gpt2-medium-vocab.j 100%[===================>]   1018K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-02-09 22:31:32 (25.2 MB/s) - ‘gpt2-medium-vocab.json’ saved [1042301/1042301]\n",
            "\n",
            "--2024-02-09 22:31:32--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.108.214, 54.231.128.72, 16.182.32.8, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.108.214|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 456318 (446K) [text/plain]\n",
            "Saving to: ‘gpt2-merges.txt’\n",
            "\n",
            "gpt2-merges.txt     100%[===================>] 445.62K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-02-09 22:31:32 (24.5 MB/s) - ‘gpt2-merges.txt’ saved [456318/456318]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers\n",
        "#This is a JSON file that contains the vocabulary (i.e., the set of words and subword pieces) used by the GPT-2 model\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-vocab.json\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Byte Pair Encoding (BPE)** is a subword tokenization technique used in natural language processing (NLP) and text processing. It involves dividing text into subword units, typically based on frequency, to create a more flexible and adaptive vocabulary for language models."
      ],
      "metadata": {
        "id": "0wdDDf6VgnwQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cyiWXf-hLtRF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "4bcaba0d-316e-4e8f-c380-069abaacd675"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[464, 2183, 286, 13630, 281, 2209, 319, 554, 7493, 3924, 3596, 2067, 351, 262, 845, 717, 554, 7493, 3924, 960, 20191, 2669, 447, 247, 82, 960, 261, 3035, 1542, 11, 1596, 4531, 13]\n",
            "['The', 'Ġcustom', 'Ġof', 'Ġdelivering', 'Ġan', 'Ġaddress', 'Ġon', 'ĠIn', 'aug', 'uration', 'ĠDay', 'Ġstarted', 'Ġwith', 'Ġthe', 'Ġvery', 'Ġfirst', 'ĠIn', 'aug', 'uration', 'âĢĶ', 'George', 'ĠWashington', 'âĢ', 'Ļ', 's', 'âĢĶ', 'on', 'ĠApril', 'Ġ30', ',', 'Ġ17', '89', '.']\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "gpt2vocab = \"gpt2-medium-vocab.json\"\n",
        "gpt2merges = \"gpt2-merges.txt\"\n",
        "\n",
        "bpe = ByteLevelBPETokenizer(gpt2vocab, gpt2merges)\n",
        "bpe_encoding = bpe.encode(\"The custom of delivering an address on Inauguration Day started with the very first Inauguration—George Washington’s—on April 30, 1789.\")\n",
        "print(bpe_encoding.ids)\n",
        "print(bpe_encoding.tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ġ represents space (' ').\n"
      ],
      "metadata": {
        "id": "UEFMVxhU4qVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4**: Collect the encoding ids which you generate from above snippet and now decode the ids to get back the given text string?\n",
        "\n",
        "**Hint**: use decode() method"
      ],
      "metadata": {
        "id": "lvqxvTMqj5qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE HERE\n",
        "print(\"Encoded tokens:\")\n",
        "print(bpe_encoding.ids)\n",
        "bpe_decoding = bpe.decode(bpe_encoding.ids)\n",
        "print()\n",
        "print(\"Decoded tokens:\")\n",
        "print(bpe_decoding)"
      ],
      "metadata": {
        "id": "GaRsld66j5Ih",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "501c4bff-f1da-4c88-d3cf-f83b3029b9d0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded tokens:\n",
            "[464, 2183, 286, 13630, 281, 2209, 319, 554, 7493, 3924, 3596, 2067, 351, 262, 845, 717, 554, 7493, 3924, 960, 20191, 2669, 447, 247, 82, 960, 261, 3035, 1542, 11, 1596, 4531, 13]\n",
            "\n",
            "Decoded tokens:\n",
            "The custom of delivering an address on Inauguration Day started with the very first Inauguration—George Washington’s—on April 30, 1789.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- decode(): Used to get back the original token. Decodes the id's to generate the original text/tokens."
      ],
      "metadata": {
        "id": "dnInB2A1_h1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Explain briefly the changes you have made in the given Tasks in Today's activity (approximately 200 words)\n",
        "-->\n"
      ],
      "metadata": {
        "id": "Bn_a90UVkv1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question-1:\n",
        "- I modified the regular expression such that it can accept only digits.\n",
        "- I used '\\d' which means any digit and '+' means the digit occurs one or more times.\n",
        "\n",
        "##Question-2:\n",
        "- I have taken the cleaned data from tutorial-2 code block-1 and applied stemming on it.\n",
        "- For Steeming there are many libraries but we have used Porter Stemmer which is from the NLTK package.\n",
        "- Stemming is nothing but reducing the terms to stem, chopping off affixes crudely.\n",
        "- In other words using fixed rules such as removing able, ing, etc. to derive the base word is called stemming.\n",
        "- Stemming can be done using NLTK but not using Spacy, because Spacy does not support Stemming, it only supports Lemmatization.\n",
        "\n",
        "##Question 3:\n",
        "- For the data that we performed stemming, not we will perform lemmatization.\n",
        "- We used WordNetLemmatizer from NLTK for lemmatization.\n",
        "It lemmatizes word by word by looping the array.\n",
        "- The clean_text is already split, so we do not need to split it again so I commented those lines.\n",
        "\n",
        "##Question 4:\n",
        "- For question 4, we decoded the tokens to get back the original data.\n",
        "- We used the encode() method to encode the sentence, which gave a unique id's for every token.\n",
        "- And if only id's are given we can use the decode() method to get the original tokens/data/sentence."
      ],
      "metadata": {
        "id": "OpHrbKGB-0MI"
      }
    }
  ]
}