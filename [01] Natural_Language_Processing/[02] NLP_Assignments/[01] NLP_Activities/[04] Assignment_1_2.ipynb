{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4b8310fc8dde43fd8a87d6aeb695d227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf7d67ca35b14644b6c4d7c936e197af",
              "IPY_MODEL_28ae9bba1ad348d4b6af804de74adf5c",
              "IPY_MODEL_ae0ae6c96bf44aecad420722eb74d21b"
            ],
            "layout": "IPY_MODEL_d62d9de56984465b953375483aba7ebd"
          }
        },
        "cf7d67ca35b14644b6c4d7c936e197af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d02214153704cd1a5eca72e5f7ceb3f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d9a5061f28db4e1799eea0bebef17266",
            "value": "Downloadingâ€‡https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:â€‡"
          }
        },
        "28ae9bba1ad348d4b6af804de74adf5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee0860e36ab44026838a6d97d1e4a852",
            "max": 46219,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a38879f21f2845f7b5b3a9ed7ed43892",
            "value": 46219
          }
        },
        "ae0ae6c96bf44aecad420722eb74d21b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20e675afb219459d938807ab57a36267",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_aba59d58867c4d4892c15f5621faf2d8",
            "value": "â€‡370k/?â€‡[00:00&lt;00:00,â€‡10.3MB/s]"
          }
        },
        "d62d9de56984465b953375483aba7ebd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d02214153704cd1a5eca72e5f7ceb3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9a5061f28db4e1799eea0bebef17266": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee0860e36ab44026838a6d97d1e4a852": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a38879f21f2845f7b5b3a9ed7ed43892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "20e675afb219459d938807ab57a36267": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aba59d58867c4d4892c15f5621faf2d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**ASSIGNMENT 1**\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "---\n",
        "* Please download the provided IPython Notebook (ipynb) file and open it in Google Colab. Once opened, enter your code in the same file directly beneath the relevant question's code block.\n",
        "* Insert a text block below your code to briefly explain it, mentioning any libraries or functions utilized. Answer the questions in brief with examples.\n",
        "\n",
        "* Submit  \n",
        "1. The IPython Notebook (ipynb) file.  \n",
        "2. A PDF version of the notebook (converted from ipynb).\n",
        "\n",
        "* The similarity score should be less than 15%"
      ],
      "metadata": {
        "id": "wknRrNHwjodK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 1: Tokenization (25%)**\n",
        "(refer to the spacy tokenization concept which is explained after Question1 in activity-1)\n",
        "\n",
        "**Question -1:**\n",
        "\n",
        "##How can we incorporate contextual information beyond individual words into the tokenization process to improve performance on downstream tasks like machine translation or question answering?"
      ],
      "metadata": {
        "id": "-0MGUnuLnXue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here:\n",
        "To do this we need a smarter way to look at words instead of just breaking them into individual words. There are different ways of doing it, some of which are as follows.\n",
        "\n",
        "1. Byte Pair Encoding (BPE): This method looks at a pair of letters that occur more frequently, combines them, and does this iteratively until a fixed size is reached. It is used in tasks such as text classification, text generation, and machine translation.\n",
        "\n",
        "2. Subword Tokenization: The words are divided into small parts which help to understand words better.\n",
        "\n",
        "3. WordPiece Tokenization: It is similar to BPE, but here it looks at the whole word at a time.\n",
        "\n",
        "4. SentencePiece Tokenization: In this method, the sentence is broken down into small parts which will help in understanding the grammar, etc."
      ],
      "metadata": {
        "id": "T9jooJ_D1UQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**Question 2**\n",
        "##Develop a tokenizer that considers contractions like \"can't\" and \"doesn't\", splitting them into their constituent words."
      ],
      "metadata": {
        "id": "VIl7PgRaeGjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"I can't believe it's not butter! You're going to love it.\"\n",
        "#CODE HERE\n",
        "# Custom contractions\n",
        "custom_contractions = {\n",
        "    \"can't\": \"can not\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"n't\": \"not\",\n",
        "    \"'s\": \" is\",\n",
        "    \"'re\": \" are\",\n",
        "    \"'ve\": \" have\",\n",
        "    \"'d\": \" would\",\n",
        "    \"'ll\": \" will\",\n",
        "}\n",
        "\n",
        "# Custom tokenizer function\n",
        "def custom_tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [custom_contractions.get(token.lower(), token) for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "# Tokenizing the text\n",
        "tokens = custom_tokenize(text)\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yoc2R_IqLwqT",
        "outputId": "a290affc-1c29-4722-d30a-e4e51cf0fb75"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'ca', 'not', 'believe', 'it', ' is', 'not', 'butter', '!', 'You', ' are', 'going', 'to', 'love', 'it', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Import nltk library for word tokenization.\n",
        "- Download Punkt from nltk.\n",
        "- Add custom mappings.\n",
        "- Create a function that tokenizes the text while considering the custom mappings as well.\n",
        "- Finally, tokenize the sentence."
      ],
      "metadata": {
        "id": "RI4_qlqVMFdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Question 3:**\n",
        "##Implement a Python script to remove Twitter username handles from a given twitter text.\n",
        "\n"
      ],
      "metadata": {
        "id": "1BRekASOnmsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Method-1"
      ],
      "metadata": {
        "id": "OgU6GA2fxwva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Great meeting with @JohnDoe and @JaneSmith today! Looking forward to our next project. Thanks for the insights @TechGuru ðŸš€ #innovation #teamwork\"\n",
        "\n",
        "words = text.split()\n",
        "cleaned_words = []\n",
        "for word in words:\n",
        "    if word.startswith(\"@\"):\n",
        "        continue\n",
        "    cleaned_words.append(word)\n",
        "\n",
        "cleaned_text = \" \".join(cleaned_words)\n",
        "\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fZv6uB3x4SUK",
        "outputId": "90dfcabe-079b-49a6-ff93-ba49b73ab889"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Great meeting with and today! Looking forward to our next project. Thanks for the insights ðŸš€ #innovation #teamwork\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The above code removes the words that start with '@' because they are Twitter usernames.\n",
        "- The above code does not use regular expressions."
      ],
      "metadata": {
        "id": "MGARiHDF4UID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method-2"
      ],
      "metadata": {
        "id": "uvm7prRtxyoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Great meeting with @JohnDoe and @JaneSmith today! Looking forward to our next project. Thanks for the insights @TechGuru ðŸš€ #innovation #teamwork\"\n",
        "#CODE HERE\n",
        "\n",
        "words = text.split()\n",
        "cleaned_words = []\n",
        "for word in words:\n",
        "    if word.startswith(\"@\"):\n",
        "        word = word[1:]\n",
        "    cleaned_words.append(word)\n",
        "\n",
        "cleaned_text = \" \".join(cleaned_words)\n",
        "\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lCqLfOf4PBYh",
        "outputId": "9aa60139-147e-4654-dbea-48ac7fc00881"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Great meeting with JohnDoe and JaneSmith today! Looking forward to our next project. Thanks for the insights TechGuru ðŸš€ #innovation #teamwork\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The above code removes the username handle '@' and replaces it with only the name.\n",
        "- This code does not use regular expressions."
      ],
      "metadata": {
        "id": "-Zw7B3Eu3_vp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Create a regular expression that identifies the Twitter username.\n",
        "- Then remove it from the sentence."
      ],
      "metadata": {
        "id": "BkvuYdfoPKmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 2.  - Regular Expressions (25%)**"
      ],
      "metadata": {
        "id": "zME8pbf7KWZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Regular Expressions**\n",
        "A regular expression, often abbreviated as regex, is a powerful and flexible tool for pattern matching and text manipulation. It consists of a sequence of characters that defines a search pattern, allowing you to perform various text-related tasks such as text validation, data extraction, text cleaning, and more. Regular expressions are used in programming languages and text editors and are constructed using a combination of regular characters, special characters, and metacharacters to specify search criteria. Learning to use regular expressions effectively can greatly enhance text processing tasks, making them a valuable skill in fields like natural language processing, data extraction, and data validation."
      ],
      "metadata": {
        "id": "tRR6kMx1haKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python includes a builtin module called `re` which provides regular expression matching operations (Click [here](https://docs.python.org/3/library/re.html) for the official module documentation). Once the module is imported into your code, you can use all of the available capabilities for performing pattern-based matching or searching using regular expressions."
      ],
      "metadata": {
        "id": "hhp7XiXXh-CQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "3L6v4jyYLh44"
      },
      "outputs": [],
      "source": [
        "##code block -1\n",
        "import re\n",
        "\n",
        "def apply_regex(data, pattern):\n",
        "  for text in data:\n",
        "    if re.fullmatch(pattern, text):\n",
        "      print(f\"Test string {text} accepted.\")\n",
        "    else:\n",
        "      print(f\"Test string {text} failed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "##**Question - 1**\n",
        "\n",
        "##Same as previous question Implement a Python script to remove Twitter username handles from a given twitter text with Regular Expression"
      ],
      "metadata": {
        "id": "ojyfJF7bymRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE HERE\n",
        "import re\n",
        "\n",
        "text = \"Great meeting with @JohnDoe and @JaneSmith today! Looking forward to our next project. Thanks for the insights @TechGuru ðŸš€ #innovation #teamwork\"\n",
        "#CODE HERE\n",
        "\n",
        "def remove_username_handles(text):\n",
        "  return re.sub(r'@\\w+', '', text)\n",
        "\n",
        "print(remove_username_handles(text))"
      ],
      "metadata": {
        "id": "wlcFDE341Idg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "44eecef9-1ac1-427b-9fdf-a535d1841b7a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Great meeting with  and  today! Looking forward to our next project. Thanks for the insights  ðŸš€ #innovation #teamwork\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The above code removes the word that starts with '@' because they are Twitter usernames.\n",
        "- It uses regular expressions to do that.\n",
        "- The regular expression \"@\\w+\" means '@' followed by one or more characters."
      ],
      "metadata": {
        "id": "bFa_46Un4nAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##**Question- 2**\n",
        "\n",
        "##Implement a Python program to find URLs in the given string."
      ],
      "metadata": {
        "id": "17YpjyzUo8oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "text= '<p>Contents :</p><a href=\"https://w3resource.com\">Python Examples</a><a href=\"http://github.com\">Even More Examples</a><a href=\"https://openai.com\">OpenAI Homepage</a><a href=\"https://docs.python.org\">Python Documentation</a>'\n",
        "##Your code here\n",
        "soup = BeautifulSoup(text, 'html.parser')\n",
        "\n",
        "links = []\n",
        "for link in soup.find_all('a'):\n",
        "    links.append(link.get('href'))\n",
        "\n",
        "print(links)"
      ],
      "metadata": {
        "id": "gRsGKmmskWJT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "98abb4f8-bd8c-4f17-c944-e753e48ab1fd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://w3resource.com', 'http://github.com', 'https://openai.com', 'https://docs.python.org']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Import BeautifulSoap\n",
        "- Parse the HTML String\n",
        "- Find all anchor tags '< a >'.\n",
        "- Extract the href attribute from each link\n",
        "- Append to a links list\n",
        "- Print the list of links"
      ],
      "metadata": {
        "id": "-xIGzV096KWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Using regular expressions based pattern matching on real world text**\n",
        "\n",
        "For the purposes of demonstration, here's a dummy paragraph of text. A few observations here:\n",
        "* The text has multiple paragraphs with each paragraph having more than one sentence.\n",
        "* Some of the words are capitalized (first letter is in uppercase followed by lowercase letters)."
      ],
      "metadata": {
        "id": "Uf1Pm9CHs1LM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Here is the First Paragraph and this is the First Sentence. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the first paragaraph. this paragraph is ending now with a Fifth Sentence.\n",
        "Now, it is the Second Paragraph and its First Sentence. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the second paragraph. this paragraph is ending now with a Fifth Sentence.\n",
        "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the third paragaraph. this paragraph is ending now with a Fifth Sentence.\n",
        "4th paragraph is not going to be detected by either of the regex patterns below.\n",
        "\"\"\"\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZPLUtLxVtiTH",
        "outputId": "c3420d6d-3c66-40ae-a2b9-fbdcab41e49c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the First Paragraph and this is the First Sentence. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the first paragaraph. this paragraph is ending now with a Fifth Sentence.\n",
            "Now, it is the Second Paragraph and its First Sentence. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the second paragraph. this paragraph is ending now with a Fifth Sentence.\n",
            "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the third paragaraph. this paragraph is ending now with a Fifth Sentence.\n",
            "4th paragraph is not going to be detected by either of the regex patterns below.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code block shows a regular expression that matches only those strings that:\n",
        "1. are at the start of a line and\n",
        "2. the string does not start with a number or a whitespace\n",
        "\n",
        "`re.findall()` finds all matches of the pattern in the text under consideration. The output is a list of strings that matched."
      ],
      "metadata": {
        "id": "zqmf8KSFtt2E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further, the regular expression defined below matches the words that are capitalized."
      ],
      "metadata": {
        "id": "UyYWQ5Tzttzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##code block - 3\n",
        "re_pattern2 = r'[A-Z][a-z]+'\n",
        "print(re.findall(re_pattern2, text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zxA3QOHHtqOg",
        "outputId": "79e5da8e-0ffc-4b9d-ab35-ed9f8c7b7380"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Here', 'First', 'Paragraph', 'First', 'Sentence', 'Second', 'Sentence', 'Third', 'Sentence', 'Fourth', 'Sentence', 'Fifth', 'Sentence', 'Now', 'Second', 'Paragraph', 'First', 'Sentence', 'Second', 'Sentence', 'Third', 'Sentence', 'Fourth', 'Sentence', 'Fifth', 'Sentence', 'Finally', 'Third', 'Paragraph', 'First', 'Sentence', 'Second', 'Sentence', 'Third', 'Sentence', 'Fourth', 'Sentence', 'Fifth', 'Sentence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Following is a text excerpt on \"Inaugural Address\" taken from the website of the [Joint Congressional Committee on Inaugural Ceremonies](https://www.inaugural.senate.gov/inaugural-address/):"
      ],
      "metadata": {
        "id": "J0VUlW3RunCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inau_text=\"\"\"The custom of delivering an address on Inauguration Day started with the very first Inaugurationâ€”George Washingtonâ€™sâ€”on April 30, 1789(04-30-1789). ex:-18.5. After taking his oath of office on the balcony of Federal Hall in New York City, Washington proceeded to the Senate chamber where he read a speech before members of Congress and other dignitaries. His second Inauguration took place in Philadelphia on March 4, 1793(03/04/1793), in the Senate chamber of Congress Hall. There, Washington gave the shortest Inaugural address on recordâ€”just 135 words â€”before repeating the oath of office.\n",
        "Every President since Washington has delivered an Inaugural address. While many of the early Presidents read their addresses before taking the oath, current custom dictates that the Chief Justice of the Supreme Court administer the oath first, followed by the Presidentâ€™s speech.\n",
        "William Henry Harrison delivered the longest Inaugural address, at 8,445 words, on March 4, 1841â€”a bitterly cold, wet day. He died one month later of pneumonia, believed to have been brought on by prolonged exposure to the elements on his Inauguration Day. John Adamsâ€™ Inaugural address, which totaled 2,308 words, contained the longest sentence, at 737 words. After Washingtonâ€™s second Inaugural address, the next shortest was Franklin D. Rooseveltâ€™s fourth address on January 20, 1945(01-20-1945), at just 559.0 words. Roosevelt had chosen to have a simple Inauguration at the White House in light of the nationâ€™s involvement in World War II.\n",
        "In 1921, Warren G. Harding became the first President to take his oath and deliver his Inaugural address through loud speakers. In 1925, Calvin Coolidgeâ€™s Inaugural address was the first to be broadcast nationally by radio. And in 1949, Harry S. Truman became the first President to deliver his Inaugural address over television airwaves.\n",
        "Most Presidents use their Inaugural address to present their vision of America and to set forth their goals for the nation. Some of the most eloquent and powerful speeches are still quoted today. In 1865, in the waning days of the Civil War, Abraham Lincoln stated, â€œWith malice toward none, with charity for all, with firmness in the right as God gives us to see the right, let us strive on to finish the work we are in, to bind up the nationâ€™s wounds, to care for him who shall have borne the battle and for his widow and his orphan, to do all which may achieve and cherish a just and lasting peace among ourselves and with all nations.â€ In 1933, Franklin D. Roosevelt avowed, â€œwe have nothing to fear but fear itself.â€ And in 1961, John F. Kennedy declared, â€œAnd so my fellow Americans: ask not what your country can do for youâ€”ask what you can do for your country.â€\n",
        "Today, Presidents deliver their Inaugural address on the West Front of the Capitol, but this has not always been the case. Until Andrew Jacksonâ€™s first Inauguration in 1829, most Presidents spoke in either the House or Senate chambers. Jackson became the first President to take his oath of office and deliver his address on the East Front Portico of the U.S. Capitol in 1829. With few exceptions, the next 37.0 Inaugurations took place there, until 1981, when Ronald Reaganâ€™s Swearing-In Ceremony and Inaugural address occurred on the West Front Terrace of the Capitol. The West Front has been used ever since. You should also need to extract the floating numbers such as -55.5, 20.8%, -3.0 using your regular expression\"\"\"\n"
      ],
      "metadata": {
        "id": "wRxR7M-NuFtR"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer to  above code block -3 for the following questions"
      ],
      "metadata": {
        "id": "0qMOPAWe3wnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##**Questions-3.A**\n",
        "## Identify all the positive and neagtive numbers with type of both intergers,float  in the \"Inaugural Address\" excerpt and write a regular expression that finds all occurrences of such words in the text. Then, run the Python code snippet to automatically display the matched strings according to the pattern.*.\n",
        "\n",
        "NOTE: You can use the *re.findall()* method as demonstrated in the example before this exercise."
      ],
      "metadata": {
        "id": "_Oap3GNEut1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "re_pattern = r'(-?\\d*\\.?\\d+%?)'\n",
        "print(re.findall(re_pattern, inau_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nSSNaw-X7bwA",
        "outputId": "9650d59b-cc22-4073-d64f-9b6191917c1e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['30', '1789', '04', '-30', '-1789', '-18.5', '4', '1793', '03', '04', '1793', '135', '8', '445', '4', '1841', '2', '308', '737', '20', '1945', '01', '-20', '-1945', '559.0', '1921', '1925', '1949', '1865', '1933', '1961', '1829', '1829', '37.0', '1981', '-55.5', '20.8%', '-3.0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- re_pattern: Regular expression pattern to match positive and negative numbers (including integers and floats)\n",
        "\n",
        "- Then find and print all occurrences of positive, negative and float point numbers in the text.\n",
        "\n"
      ],
      "metadata": {
        "id": "_6OOykzy9ZdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your explanation\n",
        "- The regular expression \"(-?\\d*\\.?\\d+%?)\" represents all the positive, negative and float numbers.\n",
        "\n",
        "- '-?' means '-' is optional.\n",
        "- '\\d*' followed by 0 or more digits.\n",
        "- '\\.?' means '.' is optional.\n",
        "- '\\d+' followed by 1 or more digits.\n",
        "- And at last, '%?' means '%' is also optional."
      ],
      "metadata": {
        "id": "IWQdvlG0dPKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Question-3.B**\n",
        "##*Identify all the dates of all forms - text form(April 20, 1945) and digit form(xx-xx-xxxx, xx/xx/xxxx) in the \"Inaugural Address\" excerpt and write a regular expression that finds all occurrences of the dates in the text. Then, run the Python code snippet to automatically display a list of all such dates identified.*\n",
        "\n",
        "NOTE: You can use the *re.findall()* method as demonstrated in the example before this exercise."
      ],
      "metadata": {
        "id": "-mkbLF4nu7la"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method-1"
      ],
      "metadata": {
        "id": "aY9fngS5yfAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Your code here\n",
        "pattern = r'\\b(\\w+ \\d{1,2}, \\d{4}|\\d{1,2}/\\d{1,2}/\\d{4}|\\d{1,2}-\\d{1,2}-\\d{4})\\b'\n",
        "print(re.findall(pattern, inau_text))"
      ],
      "metadata": {
        "id": "2pxl72VhvEcG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "84e30ae1-f65b-48df-e8f1-fa09ae509d8b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['April 30, 1789', '04-30-1789', 'March 4, 1793', '03/04/1793', 'March 4, 1841', 'January 20, 1945', '01-20-1945']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method-2"
      ],
      "metadata": {
        "id": "vmyvvN8DyhNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "re_pattern = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}|(?:\\d{1,2}[-/]\\d{1,2}[-/]\\d{4})'\n",
        "print(re.findall(re_pattern, inau_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BEL65N1h-6XH",
        "outputId": "34b33257-404b-40e0-d610-956f7b74ecd9"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['April 30, 1789', '04-30-1789', 'March 4, 1793', '03/04/1793', 'March 4, 1841', 'January 20, 1945', '01-20-1945']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The above regular expression finds the dates in all formats.\n",
        "- Then print and find all the occurrences of the regular expression in the text.\n"
      ],
      "metadata": {
        "id": "DxhgOUqb_BKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your explanation\n",
        "- We can write 2 regular expressions for date formats.\n",
        "- The first regular expression is a generic regular expression that does not care about the spelling of the month.\n",
        "Whereas the second regular expression checks for the spelling as well, which is a more specific regular expression."
      ],
      "metadata": {
        "id": "TdUT4vfidQ_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 3: Lemmatization/Stemming(25%)**\n",
        "\n",
        "\n",
        "#**Question -1:**\n",
        "##How does the morphology of a language (e.g., agglutinative vs. fusional) impact the suitability of stemming vs. lemmatization?"
      ],
      "metadata": {
        "id": "QZSO1IrN06xI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your answer here"
      ],
      "metadata": {
        "id": "80ycUYp41Iy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The structure of a word in the language is known as the morphology of a language.\n",
        "The morphology of a language has a great impact on the suitability of stemming and lemmatization.\n",
        "- Stemming: It is adding or removing prefixes and suffixes. Stemming does not require one to understand the language, instead, it follows a set of pre-defined rules.\n",
        "- Lemmatization: It involves identifying a lemma that requires a proper understanding of the language.\n",
        "\n",
        "## Agglutinative languages\n",
        "- Agglutinative languages like Turkish and Finnish forms words by combining morphemes, which are small units of meanings. For example, the Turkish word \"evlerinizdekilerden\" (meaning \"from those at your house\") is built from smaller parts like \"ev\" (house), \"ler\" (plural marker), \"iniz\" (your).\n",
        "- When considering agglutinate language stemming is more effective when compared with lemmatization because words are assembled from distinct morphenes.\n",
        "\n",
        "## Fusional languages\n",
        "- Fusional languages like French and Spanish often change the morphemes and the form which requires language understanding to understand it.\n",
        "- For example, the French verb \"parler\" (to speak) becomes \"je parle\" (I speak) by merging the pronoun and tense.\n",
        "- So, in case fusional language stemming will not work as effectively as lemmatization will.\n",
        "- Lemmatization uses more complex ways to map the words to its base word."
      ],
      "metadata": {
        "id": "62oiVnX8K0di"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        " ## **Question - 2 :**\n",
        "\n",
        "\n",
        "## Create a Python function that takes a sentence as input, performs lemmatization using **StanfordNLP**, and removes stopwords from the lemmatized sentence. Use a list of stopwords. Return the cleaned and lemmatized sentence.\n",
        "\n",
        "Reference:https://stanfordnlp.github.io/stanfordnlp/\n",
        "\n"
      ],
      "metadata": {
        "id": "EMMf51DdsaK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ekg11Mv_d-wl",
        "outputId": "30378146-edf6-4ca8-cdf5-33630e20497f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stanza in /usr/local/lib/python3.10/dist-packages (1.7.0)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from stanza) (2.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.31.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from stanza) (3.2.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from stanza) (0.10.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.66.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Import Stanza"
      ],
      "metadata": {
        "id": "7Wmai9gHexcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YeFi_lskeRcx",
        "outputId": "9dabb342-6cda-40bf-ca33-c79d8f0fb939"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Download all the stopwords"
      ],
      "metadata": {
        "id": "FgyTnnrRe1BY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords = stopwords.words('english')\n",
        "\n",
        "def lemmatize_and_remove_stopwords(sentence):\n",
        "  nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')\n",
        "  doc = nlp(sentence)\n",
        "\n",
        "  lemmatized = [word.lemma for sent in doc.sentences for word in sent.words]\n",
        "\n",
        "  cleaned = [word for word in lemmatized if word not in stopwords]\n",
        "\n",
        "  return \" \".join(cleaned)"
      ],
      "metadata": {
        "id": "8GPS0THMdoQS"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Create a methods to remove stop words and lemmatization."
      ],
      "metadata": {
        "id": "yN2nFYv9e4ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"\"\"The custom of delivering an address on Inauguration Day started with the very first Inaugurationâ€”George Washingtonâ€™sâ€”on April 30, 1789(04-30-1789). ex:-18.5. After taking his oath of office on the balcony of Federal Hall in New York City, Washington proceeded to the Senate chamber where he read a speech before members of Congress and other dignitaries. His second Inauguration took place in Philadelphia on March 4, 1793(03/04/1793), in the Senate chamber of Congress Hall. There, Washington gave the shortest Inaugural address on recordâ€”just 135 words â€”before repeating the oath of office.\n",
        "Every President since Washington has delivered an Inaugural address. While many of the early Presidents read their addresses before taking the oath, current custom dictates that the Chief Justice of the Supreme Court administer the oath first, followed by the Presidentâ€™s speech.\n",
        "William Henry Harrison delivered the longest Inaugural address, at 8,445 words, on March 4, 1841â€”a bitterly cold, wet day. He died one month later of pneumonia, believed to have been brought on by prolonged exposure to the elements on his Inauguration Day. John Adamsâ€™ Inaugural address, which totaled 2,308 words, contained the longest sentence, at 737 words. After Washingtonâ€™s second Inaugural address, the next shortest was Franklin D. Rooseveltâ€™s fourth address on January 20, 1945(01-20-1945), at just 559.0 words. Roosevelt had chosen to have a simple Inauguration at the White House in light of the nationâ€™s involvement in World War II.\n",
        "In 1921, Warren G. Harding became the first President to take his oath and deliver his Inaugural address through loud speakers. In 1925, Calvin Coolidgeâ€™s Inaugural address was the first to be broadcast nationally by radio. And in 1949, Harry S. Truman became the first President to deliver his Inaugural address over television airwaves.\n",
        "Most Presidents use their Inaugural address to present their vision of America and to set forth their goals for the nation. Some of the most eloquent and powerful speeches are still quoted today. In 1865, in the waning days of the Civil War, Abraham Lincoln stated, â€œWith malice toward none, with charity for all, with firmness in the right as God gives us to see the right, let us strive on to finish the work we are in, to bind up the nationâ€™s wounds, to care for him who shall have borne the battle and for his widow and his orphan, to do all which may achieve and cherish a just and lasting peace among ourselves and with all nations.â€ In 1933, Franklin D. Roosevelt avowed, â€œwe have nothing to fear but fear itself.â€ And in 1961, John F. Kennedy declared, â€œAnd so my fellow Americans: ask not what your country can do for youâ€”ask what you can do for your country.â€\n",
        "Today, Presidents deliver their Inaugural address on the West Front of the Capitol, but this has not always been the case. Until Andrew Jacksonâ€™s first Inauguration in 1829, most Presidents spoke in either the House or Senate chambers. Jackson became the first President to take his oath of office and deliver his address on the East Front Portico of the U.S. Capitol in 1829. With few exceptions, the next 37.0 Inaugurations took place there, until 1981, when Ronald Reaganâ€™s Swearing-In Ceremony and Inaugural address occurred on the West Front Terrace of the Capitol. The West Front has been used ever since. You should also need to extract the floating numbers such as -55.5, 20.8%, -3.0 using your regular expression\"\"\"\n",
        "print(lemmatize_and_remove_stopwords(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390,
          "referenced_widgets": [
            "4b8310fc8dde43fd8a87d6aeb695d227",
            "cf7d67ca35b14644b6c4d7c936e197af",
            "28ae9bba1ad348d4b6af804de74adf5c",
            "ae0ae6c96bf44aecad420722eb74d21b",
            "d62d9de56984465b953375483aba7ebd",
            "9d02214153704cd1a5eca72e5f7ceb3f",
            "d9a5061f28db4e1799eea0bebef17266",
            "ee0860e36ab44026838a6d97d1e4a852",
            "a38879f21f2845f7b5b3a9ed7ed43892",
            "20e675afb219459d938807ab57a36267",
            "aba59d58867c4d4892c15f5621faf2d8"
          ]
        },
        "id": "Ei8G1R96eS-w",
        "outputId": "b6899834-485f-479d-8c7d-f517e016299b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   â€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b8310fc8dde43fd8a87d6aeb695d227"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Loading these models for language: en (English):\n",
            "=================================\n",
            "| Processor | Package           |\n",
            "---------------------------------\n",
            "| tokenize  | combined          |\n",
            "| mwt       | combined          |\n",
            "| pos       | combined_charlm   |\n",
            "| lemma     | combined_nocharlm |\n",
            "=================================\n",
            "\n",
            "INFO:stanza:Using device: cpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: mwt\n",
            "INFO:stanza:Loading: pos\n",
            "INFO:stanza:Loading: lemma\n",
            "INFO:stanza:Done loading processors!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom deliver address Inauguration Day start first Inauguration â€” George Washington 's â€” April 30 , 179( 04-30-1789 ) . ex :- 18.5 . take oath office balcony Federal Hall New York City , Washington proceed Senate chamber read speech member Congress dignitary . second Inauguration take place Philadelphia March 4 , 1793 ( 03/04/1793 ) , Senate chamber Congress Hall . , Washington give short inaugural address record â€” 135 word â€” repeat oath office . every President since Washington deliver inaugural address . many early president read address take oath , current custom dictate Chief Justice Supreme Court administer oath first , follow president 's speech . William Henry Harrison deliver long inaugural address , 8,445 word , March 4 , 1841 â€” bitterly cold , wet day . die one month late pneumonia , believe bring prolonged exposure element Inauguration Day . John Adams 's inaugural address , total 2308 word , contain long sentence , 737 word . Washington 's second inaugural address , next short Franklin D. Roosevelt 's fourth address January 20 , 1945 ( 01-20 - 1945 ) , 559.0 word . Roosevelt choose simple inauguration White House light nation 's involvement World War II . 1921 , Warren G. Harding become first President take oath deliver inaugural address loud speaker . 1925 , Calvin Coolidge 's inaugural address first broadcast nationally radio . 1949 , Harry S. Truman become first President deliver inaugural address television airwave . president use inaugural address present vision America set forth goal nation . eloquent powerful speech still quote today . 1865 , wane day Civil War , Abraham Lincoln state , '' malice toward none , charity , firmness right God give see right , let strive finish work , bind nation 's wound , care shall borne battle widow orphan , may achieve cherish lasting peace among nation . '' 1933 , Franklin D. Roosevelt avow , '' nothing fear fear . '' 1961 , John F. Kennedy declare , '' fellow American : ask country â€” ask country . '' today , president deliver inaugural address West Front Capitol , always case . Andrew Jackson 's first Inauguration 1829 , president speak either House Senate chamber . Jackson become first President take oath office deliver address East Front Portico U.S. Capitol 1829 . exception , next 37.0 inauguration take place , 1981 , Ronald Reagan 's Swearing - Ceremony inaugural address occur West Front Terrace Capitol . West Front use ever since . also need extract float number - 55.5 , 20.8 % , - 3.0 use regular expression\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This is an example sentence."
      ],
      "metadata": {
        "id": "VPy_uSvTSLwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your answer here\n",
        "\n",
        "- First, install, import and download all the required libraries\n",
        "- Download the Stanza library and stop words from NLTK.\n",
        "- Then I wrote a method that does the following tasks\n",
        "    - Use the Stanza pipeline method to create a pipeline that handles running multiple NLP annotation tasks on text.\n",
        "    - Lemmatize the words and store it in lemmatization.\n",
        "    - Remove stopwords and store them in cleaned.\n",
        "    - Join the lemmatized words into a sentence and return it.\n",
        "- Finally, I used an example sentence to test the method and the output."
      ],
      "metadata": {
        "id": "Ri0OIiWfBN8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Question - 3**\n",
        "\n",
        "(refer to the byte pair encoding concept which is explained in activity-2 at the Tutorial of Subword Tokenization using HuggingFace after the Question6 in activity-2 )\n",
        "\n",
        "\n",
        "Consider the following two sentences:\n",
        "\n",
        "**S1:**I like yellow roses better than red ones.\n",
        "\n",
        "**S2:**Looks like John is bettering the working conditions at his organization\n",
        "\n",
        "**Create a Python function that encodes two sentences using the custom BPE tokenizer and identifies common subword tokens (tokens that appear in both encodings). Return a list of these common subword tokens. Is/Are there any interesting observations when you compare the tokens between the two encodings? What do you think is causing what you observe as part of your comparison?**"
      ],
      "metadata": {
        "id": "I7o1E0wTupIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method-1"
      ],
      "metadata": {
        "id": "u8mM023wzA_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code here\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "def find_common_subwords(sentence1, sentence2):\n",
        "    # Initialization of BERT tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Tokenizing the sentences using the BERT tokenizer\n",
        "    encoded_sentence1 = tokenizer.tokenize(sentence1)\n",
        "    encoded_sentence2 = tokenizer.tokenize(sentence2)\n",
        "\n",
        "    # Finding the common subwords\n",
        "    common_subwords = set(encoded_sentence1) & set(encoded_sentence2)\n",
        "\n",
        "    return list(common_subwords)\n",
        "\n",
        "# Example usage\n",
        "sentence1 = \"I like yellow roses better than red ones.\"\n",
        "sentence2 = \"Looks like John is bettering the working conditions at his organization\"\n",
        "common_subwords = find_common_subwords(sentence1, sentence2)\n",
        "print(\"Common subwords:\", common_subwords)\n"
      ],
      "metadata": {
        "id": "3RY65-RPvURG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b2145023-3067-40ee-f709-2d3bfc9d1aab"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common subwords: ['better', 'like']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method-2"
      ],
      "metadata": {
        "id": "aEaUPJKnzCoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers\n",
        "#This is a JSON file that contains the vocabulary (i.e., the set of words and subword pieces) used by the GPT-2 model\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-vocab.json\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YNEqmBh2ZPN_",
        "outputId": "2aa37890-3fc3-4cd0-facc-1692d0b7a5f7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2024.2.2)\n",
            "--2024-02-22 17:14:32--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-vocab.json\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.115.248, 52.216.44.96, 52.217.172.144, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.115.248|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1042301 (1018K) [application/json]\n",
            "Saving to: â€˜gpt2-medium-vocab.json.1â€™\n",
            "\n",
            "gpt2-medium-vocab.j 100%[===================>]   1018K  2.68MB/s    in 0.4s    \n",
            "\n",
            "2024-02-22 17:14:32 (2.68 MB/s) - â€˜gpt2-medium-vocab.json.1â€™ saved [1042301/1042301]\n",
            "\n",
            "--2024-02-22 17:14:33--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.115.248, 52.216.44.96, 52.217.172.144, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.115.248|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 456318 (446K) [text/plain]\n",
            "Saving to: â€˜gpt2-merges.txt.1â€™\n",
            "\n",
            "gpt2-merges.txt.1   100%[===================>] 445.62K  1.47MB/s    in 0.3s    \n",
            "\n",
            "2024-02-22 17:14:33 (1.47 MB/s) - â€˜gpt2-merges.txt.1â€™ saved [456318/456318]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Install the tokenizer"
      ],
      "metadata": {
        "id": "dFn54-UgamgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "import requests\n",
        "\n",
        "def download_file(url, file_name):\n",
        "    response = requests.get(url)\n",
        "    with open(file_name, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "def find_common_subwords(sentence1, sentence2):\n",
        "    # Download vocabulary and merges files\n",
        "    gpt2vocab_url = \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-vocab.json\"\n",
        "    gpt2merges_url = \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\"\n",
        "    download_file(gpt2vocab_url, \"gpt2-medium-vocab.json\")\n",
        "    download_file(gpt2merges_url, \"gpt2-merges.txt\")\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    bpe = ByteLevelBPETokenizer(\"gpt2-medium-vocab.json\", \"gpt2-merges.txt\")\n",
        "\n",
        "    # Encode the sentences\n",
        "    encoding1 = bpe.encode(sentence1)\n",
        "    encoding2 = bpe.encode(sentence2)\n",
        "\n",
        "    # Find common subword tokens\n",
        "    common_tokens = set(encoding1.tokens).intersection(encoding2.tokens)\n",
        "\n",
        "    # Decode common subword tokens and get their corresponding encoded IDs\n",
        "    decoded_common_tokens = [bpe.decode([bpe.token_to_id(token)]) for token in common_tokens]\n",
        "    encoded_common_tokens = [bpe.encode(token).ids[0] for token in decoded_common_tokens]\n",
        "\n",
        "    return decoded_common_tokens, encoded_common_tokens\n",
        "\n",
        "# Example sentences\n",
        "sentence1 = \"I like yellow roses better than red ones.\"\n",
        "sentence2 = \"Looks like John is bettering the working conditions at his organization\"\n",
        "\n",
        "# Find and print both encoded and decoded words for common subword tokens\n",
        "decoded_common_subwords, encoded_common_subwords = find_common_subwords(sentence1, sentence2)\n",
        "print(\"Common Subword Tokens (Encoded):\", encoded_common_subwords)\n",
        "print(\"Common Subword Tokens (Decoded):\", decoded_common_subwords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "o3ZCj9djZBk7",
        "outputId": "cab7009a-ab1f-4f91-8584-1a569befa05f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common Subword Tokens (Encoded): [1365, 588]\n",
            "Common Subword Tokens (Decoded): [' better', ' like']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- First, download, install and import the files and libraries required.\n",
        "- Define the method/function that extracts the common subwords. The method does the following things.\n",
        "  - Download the vocabulary of huggyface, and merge it.\n",
        "  - Initialize ByteLevelBPETokenizer.\n",
        "  - Encode the given sentences.\n",
        "  - Then finding the common sub-words.\n",
        "  - Finally, decode it.\n",
        "- Print the encoded and decoded common sub-words"
      ],
      "metadata": {
        "id": "_LOstl6badfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task - 4 : Minimum Edit distance (25%)**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##Minimum edit Distance\n",
        "Minimum Edit Distance (also known as Levenshtein Distance) is a measure of similarity between two strings by calculating the minimum number of single-character edits (insertions, deletions, substitutions) required to transform one string into the other. It has applications in various fields, including natural language processing, spell checking, DNA sequence alignment, and more.\n",
        "\n"
      ],
      "metadata": {
        "id": "IEKqJAYbvUG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Character Based Text Similarity\n",
        "\"As an example, this technology is used by information retrieval systems, search engines, automatic indexing systems, text summarizers, categorization systems, plagiarism checkers, speech recognition, rating systems, DNA analysis, and profiling algorithms (IR/AI programs to automatically link data between people and what they do).\""
      ],
      "metadata": {
        "id": "aNZzSXu2xoAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##code block -4\n",
        "# A Naive recursive Python program to find minimum number\n",
        "# operations to convert str1 to str2\n",
        "\n",
        "\n",
        "def editDistance(str1, str2, m, n):\n",
        "\n",
        "    # If first string is empty, the only option is to\n",
        "    # insert all characters of second string into first\n",
        "    if m == 0:\n",
        "        return n\n",
        "\n",
        "    # If second string is empty, the only option is to\n",
        "    # remove all characters of first string\n",
        "    if n == 0:\n",
        "        return m\n",
        "\n",
        "    # If last characters of two strings are same, nothing\n",
        "    # much to do. Ignore last characters and get count for\n",
        "    # remaining strings.\n",
        "    if str1[m-1] == str2[n-1]:\n",
        "        return editDistance(str1, str2, m-1, n-1)\n",
        "\n",
        "    # If last characters are not same, consider all three\n",
        "    # operations on last character of first string, recursively\n",
        "    # compute minimum cost for all three operations and take\n",
        "    # minimum of three values.\n",
        "    return 1 + min(editDistance(str1, str2, m, n-1),    # Insert\n",
        "                   editDistance(str1, str2, m-1, n),    # Remove\n",
        "                   editDistance(str1, str2, m-1, n-1)    # Replace\n",
        "                   )\n",
        "\n",
        "\n",
        "# Driver code\n",
        "str1 = \"sunday\"\n",
        "str2 = \"saturday\"\n",
        "print (editDistance(str1, str2, len(str1), len(str2)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yHA_ccV_xmCB",
        "outputId": "2dbc6d84-236a-485a-ebca-62b135f1b0b2"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer above code block -4  for the following question"
      ],
      "metadata": {
        "id": "nSfpOff4Ngbx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question-1\n",
        "##Assuming case sensitivity where changing a letter's case has a cost of 1, calculate the minimum cost to transform \"Imagine\" into \"imagination\" with the following operation costs: insertions = 2, deletions = 2, substitutions = 3"
      ],
      "metadata": {
        "id": "OUJ1n35-dgON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##your code here\n",
        "# Operation costs\n",
        "insertions = 2\n",
        "deletions = 2\n",
        "substitutions = 3\n",
        "\n",
        "def editDistance(str1, str2, m, n):\n",
        "\n",
        "  # If first string is empty, insert all chars of second string\n",
        "  if m == 0:\n",
        "    return n * insertions\n",
        "\n",
        "  # If second string is empty, delete all chars of first string\n",
        "  if n == 0:\n",
        "    return m * deletions\n",
        "\n",
        "  # If last characters are same, ignore them and recur for remaining strings\n",
        "  if str1[m-1] == str2[n-1]:\n",
        "    return editDistance(str1, str2, m-1, n-1)\n",
        "\n",
        "  # Consider all possibilities and take minimum\n",
        "  return min(\n",
        "      editDistance(str1, str2, m, n-1) + insertions,\n",
        "      editDistance(str1, str2, m-1, n) + deletions,\n",
        "      editDistance(str1, str2, m-1, n-1) + (substitutions if str1[m-1] != str2[n-1] else 0)\n",
        "  )\n",
        "\n",
        "str1 = \"Imagine\"\n",
        "str2 = \"imagination\"\n",
        "print(editDistance(str1, str2, len(str1), len(str2)))"
      ],
      "metadata": {
        "id": "bt4ZjDL2c3LB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "08927168-debb-4b97-8dd0-41522c499d1f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your explanation"
      ],
      "metadata": {
        "id": "6fdCkNgZdL7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The edit distance function takes 4 inputs - string 1, its length, string 2, and its length.\n",
        "- Cost of insertion, deletion, and substitution are already fixed and initialized.\n",
        "- If string-1 is empty then insert all the characters to string-2, which means we need to insert all the characters from string-1, so the cost will be the length of string-1 multiplied by the insertion cost.\n",
        "- If string-2 is empty, we need to delete all characters from string-1 which will cost length of string-2 multiplied by deletion cost.\n",
        "- If the last characters are the same, in that case, we can ignore and recursively calculate for the remaining string.\n",
        "- Finally, we need to perform 3 operations on the last character of strig-1 - insert, delete, and substitute.\n",
        "- Then return the minimum of 3 values.\n",
        "\n",
        "##Example\n",
        "String-1: Imagine\n",
        "\n",
        "String-2: imagination\n",
        "\n",
        "i -> I : Substitution = 3\n",
        "\n",
        "e -> a : Substitution = 3\n",
        "\n",
        "t : Insertion = 2\n",
        "\n",
        "i : Insertion = 2\n",
        "\n",
        "o : Insertion = 2\n",
        "\n",
        "n : Insertion = 2\n",
        "\n",
        "Total = 14"
      ],
      "metadata": {
        "id": "VeFXwhqgg3X9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tutorial -2\n",
        "# Levenshtein Distance for Sentences"
      ],
      "metadata": {
        "id": "RFiLMn7rIHUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code block - 5\n",
        "# Simple Minimum Edit Distance\n",
        "def levenshtein_distance(str1, str2):\n",
        "    # Initialize a matrix to store edit distances\n",
        "    m, n = len(str1), len(str2)\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    # Initialize the first row and column\n",
        "    for i in range(m + 1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(n + 1):\n",
        "        dp[0][j] = j\n",
        "\n",
        "    # Fill in the matrix using dynamic programming\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            cost = 0 if str1[i - 1] == str2[j - 1] else 2  # Substitution cost is 2\n",
        "            dp[i][j] = min(\n",
        "                dp[i - 1][j] + 1,  # Deletion\n",
        "                dp[i][j - 1] + 1,  # Insertion\n",
        "                dp[i - 1][j - 1] + cost,  # Substitution\n",
        "            )\n",
        "\n",
        "    # The final value in the matrix represents the Levenshtein distance\n",
        "    return dp[m][n]\n",
        "\n",
        "# Example usage\n",
        "str1 = \"This is a cat\"\n",
        "str2 = \"That is a dog\"\n",
        "distance = levenshtein_distance(str1, str2)\n",
        "print(f\"The Levenshtein distance between '{str1}' and '{str2}' with substitution cost 2 is {distance}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FypLc-dkO96G",
        "outputId": "490a16d3-aacb-4736-efd4-0a789b273822"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Levenshtein distance between 'This is a cat' and 'That is a dog' with substitution cost 2 is 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer to above code block -5 from tutorial - 2 for the following question"
      ],
      "metadata": {
        "id": "e-xrzKVC5iSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question:2\n",
        "##Assign different costs to insertions, deletions, and substitutions to reflect varying penalties for different types of edits. Calculate the Levenshtein distance with these weighted costs.\n",
        "\n",
        "String1 = (\"Natural language processing\")\n",
        "\n",
        "String2 = (\"Computer science department\")\n",
        "\n",
        "Provide your explanation in the tex block below"
      ],
      "metadata": {
        "id": "EapfLAq5iCLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##ENTER YOUR CODE HERE\n",
        "# Weighted costs\n",
        "INSERTION_COST = 2\n",
        "DELETION_COST = 3\n",
        "SUBSTITUTION_COST = 4\n",
        "\n",
        "def levenshtein_distance(str1, str2):\n",
        "\n",
        "  m, n = len(str1), len(str2)\n",
        "\n",
        "  # Initialize a matrix to store edit distances\n",
        "  dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "  # Initialize first row and column\n",
        "  for i in range(m + 1):\n",
        "    dp[i][0] = i * DELETION_COST\n",
        "  for j in range(n + 1):\n",
        "    dp[0][j] = j * INSERTION_COST\n",
        "\n",
        "  # Fill in the matrix using dynamic programming\n",
        "  for i in range(1, m + 1):\n",
        "    for j in range(1, n + 1):\n",
        "      if str1[i-1] == str2[j-1]:\n",
        "        cost = 0\n",
        "      else:\n",
        "        cost = SUBSTITUTION_COST\n",
        "\n",
        "      dp[i][j] = min(\n",
        "        dp[i-1][j] + DELETION_COST,\n",
        "        dp[i][j-1] + INSERTION_COST,\n",
        "        dp[i-1][j-1] + cost)\n",
        "\n",
        "  return dp[m][n]\n",
        "\n",
        "# Example usage\n",
        "str1 = \"Natural language processing\"\n",
        "str2 = \"Computer science department\"\n",
        "\n",
        "print(levenshtein_distance(str1, str2))"
      ],
      "metadata": {
        "id": "kcDvlYTD4YvR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "bc48410b-59d9-4694-b7a3-a59e61d0dd62"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your explanation"
      ],
      "metadata": {
        "id": "texg77AFdG6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Standard Leveshtein distance uses cost as 1 for all operations, but in the above code, I have assigned different costs for different operations.\n",
        "    - Insertion cost = 2\n",
        "    - Deletion cost = 3\n",
        "    - Substitution cost = 4\n",
        "- levenshtein_distance is a method that takes 2 inputs - string-1 and string-2\n",
        "- We filled up dynamic programming matrix, insted of increating by 1, I increaed with the particular cost assigned.\n",
        "- The levenshtein_distance for the example string is 76."
      ],
      "metadata": {
        "id": "VgjqeWsmnv9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question-3\n",
        "##Describe how MED is used in various NLP tasks, such as spell checking, speech recognition, text summarization, and machine translation. How does its effectiveness vary across these tasks?"
      ],
      "metadata": {
        "id": "yLV2Db4O0pj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your Explanation Here\n",
        "--> The below are the ways MED (Minimum Edit Distance) is used in various natural language processing tasks.\n",
        "## 1. Spell checking:\n",
        "- MED helps in finding the correct spelling by looking for the word that requires fewer edits to change to the misspelled word. It is very useful in finding out the typos.\n",
        "\n",
        "## 2. Speech Recognition:\n",
        "- Sometimes the device does not understand the word that you told. So in that case it compares the word you told and the word with the word what it thinks you meant. It finds the word that required smallest number of changes and it picks that word\n",
        "\n",
        "## 3. Text Summarization:\n",
        "- Text summarization means shortening the paragraph/text without losing the main content. To do so MSD helps in picking the important sentence by looking how familier it is with the other sentence. If two sentences are similar then you need only one in the summary.\n",
        "\n",
        "## 4. Machine Translation:\n",
        "- MSE is also used in machine translation. When converting the text from one language to another, it finds the best word/sentence/phrase to use in the other language by comparing it with the original and translated sentence and finds the smallest number of changes required.\n"
      ],
      "metadata": {
        "id": "jfyAF_SacJZd"
      }
    }
  ]
}